{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4848574a",
   "metadata": {},
   "source": [
    "# SEO Content Detection Pipeline\n",
    "\n",
    "This notebook implements a pipeline for detecting SEO-generated content and duplicates. It includes:\n",
    "1. Content extraction from HTML\n",
    "2. Feature engineering\n",
    "3. Quality analysis\n",
    "4. Duplicate detection\n",
    "5. Model training and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcb630b",
   "metadata": {},
   "source": [
    "# 1. Setup Environment and Load Libraries\n",
    "\n",
    "First, we'll import all required libraries and set up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec312831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading required NLTK resources...\n",
      "✓ Successfully downloaded punkt\n",
      "✓ Successfully downloaded stopwords\n",
      "✓ Successfully downloaded wordnet\n",
      "✓ Successfully downloaded omw-1.4\n",
      "✓ Successfully downloaded averaged_perceptron_tagger\n",
      "✓ Successfully downloaded averaged_perceptron_tagger_eng\n",
      "✓ Successfully downloaded punkt_tab\n",
      "\n",
      "NLTK setup completed!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, classification_report, ConfusionMatrixDisplay\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.inspection import permutation_importance\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import pickle\n",
    "import joblib\n",
    "import textstat\n",
    "from collections import Counter\n",
    "import os\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "# Download all required NLTK data (cover new resource names as well)\n",
    "print(\"Downloading required NLTK resources...\")\n",
    "required_packages = [\n",
    "    'punkt',                         # Tokenization\n",
    "    'stopwords',                     # Stopwords\n",
    "    'wordnet',                       # Lemmatization\n",
    "    'omw-1.4',                       # Open Multilingual Wordnet\n",
    "    'averaged_perceptron_tagger',    # POS tagger (legacy name)\n",
    "    'averaged_perceptron_tagger_eng' # POS tagger (newer resource name)\n",
    "]\n",
    "\n",
    "for package in required_packages:\n",
    "    try:\n",
    "        nltk.download(package, quiet=True)\n",
    "        print(f\"✓ Successfully downloaded {package}\")\n",
    "    except Exception as e:\n",
    "        print(f\"× Error downloading {package}: {str(e)}\")\n",
    "\n",
    "# Some environments also require 'punkt_tab'\n",
    "try:\n",
    "    nltk.download('punkt_tab', quiet=True)\n",
    "    print(\"✓ Successfully downloaded punkt_tab\")\n",
    "except Exception as e:\n",
    "    print(f\"· Skipped punkt_tab: {str(e)}\")\n",
    "\n",
    "print(\"\\nNLTK setup completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008288da",
   "metadata": {},
   "source": [
    "# 2. Load and Parse HTML Content\n",
    "\n",
    "Now we'll load the raw HTML content from data.csv and extract clean text content using BeautifulSoup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "17636cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing URLs...\n",
      "Processing 1/81 URLs...\n",
      "Processing 11/81 URLs...\n",
      "Processing 21/81 URLs...\n",
      "Processing 31/81 URLs...\n",
      "Processing 41/81 URLs...\n",
      "Processing 51/81 URLs...\n",
      "Processing 61/81 URLs...\n",
      "Processing 71/81 URLs...\n",
      "Processing 81/81 URLs...\n",
      "\n",
      "Content extraction completed and saved to extracted_content.csv\n",
      "\n",
      "Extraction Statistics:\n",
      "Total URLs processed: 81\n",
      "URLs with empty content: 13\n",
      "URLs with titles: 58\n",
      "URLs with meta descriptions: 0\n"
     ]
    }
   ],
   "source": [
    "# Load raw data\n",
    "data_df = pd.read_csv('/Users/ajanshul02gmail.com/Start-up/Walnut/seo-content-detector/data/data.csv')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Comprehensive text preprocessing function\n",
    "    \"\"\"\n",
    "    if pd.isna(text):  # Handle NaN values\n",
    "        return ''\n",
    "        \n",
    "    # Convert to string if not already\n",
    "    text = str(text)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove HTML tags (if any remaining)\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www.\\S+', '', text)\n",
    "    \n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    \n",
    "    # Rejoin tokens\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def extract_content(html):\n",
    "    \"\"\"\n",
    "    Extract and clean content from HTML\n",
    "    \"\"\"\n",
    "    if pd.isna(html):  # Handle NaN values\n",
    "        return '', '', ''\n",
    "        \n",
    "    try:\n",
    "        soup = BeautifulSoup(str(html), 'html.parser')\n",
    "        \n",
    "        # Remove script and style elements\n",
    "        for element in soup(['script', 'style', 'meta', 'link']):\n",
    "            element.decompose()\n",
    "        \n",
    "        # Extract title\n",
    "        title = soup.title.text if soup.title else ''\n",
    "        \n",
    "        # Extract meta description\n",
    "        meta_desc = ''\n",
    "        meta_tag = soup.find('meta', attrs={'name': 'description'})\n",
    "        if meta_tag:\n",
    "            meta_desc = meta_tag.get('content', '')\n",
    "        \n",
    "        # Extract main content\n",
    "        # Focus on specific content tags\n",
    "        content_tags = soup.find_all(['p', 'article', 'div', 'section'])\n",
    "        main_content = ' '.join(tag.get_text() for tag in content_tags)\n",
    "        \n",
    "        # Clean extracted text\n",
    "        main_content = preprocess_text(main_content)\n",
    "        title = preprocess_text(title)\n",
    "        meta_desc = preprocess_text(meta_desc)\n",
    "        \n",
    "        return title, main_content, meta_desc\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing HTML content: {str(e)}\")\n",
    "        return '', '', ''\n",
    "\n",
    "# Process each URL and save results\n",
    "print(\"Processing URLs...\")\n",
    "extracted_data = []\n",
    "total_urls = len(data_df)\n",
    "\n",
    "for idx, row in data_df.iterrows():\n",
    "    if idx % 10 == 0:  # Print progress every 10 items\n",
    "        print(f\"Processing {idx+1}/{total_urls} URLs...\")\n",
    "        \n",
    "    title, content, meta = extract_content(row['html_content'])\n",
    "    extracted_data.append({\n",
    "        'url': row['url'],\n",
    "        'title': title,\n",
    "        'main_content': content,\n",
    "        'meta_description': meta\n",
    "    })\n",
    "\n",
    "# Save extracted content\n",
    "extracted_df = pd.DataFrame(extracted_data)\n",
    "extracted_df.to_csv('../data/extracted_content.csv', index=False)\n",
    "print(\"\\nContent extraction completed and saved to extracted_content.csv\")\n",
    "\n",
    "# Print statistics\n",
    "print(\"\\nExtraction Statistics:\")\n",
    "print(f\"Total URLs processed: {len(extracted_df)}\")\n",
    "print(f\"URLs with empty content: {len(extracted_df[extracted_df['main_content'] == ''])}\")\n",
    "print(f\"URLs with titles: {len(extracted_df[extracted_df['title'] != ''])}\")\n",
    "print(f\"URLs with meta descriptions: {len(extracted_df[extracted_df['meta_description'] != ''])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6debd34f",
   "metadata": {},
   "source": [
    "# 3. Extract Text Features\n",
    "\n",
    "We'll extract various features from the text content to help identify SEO-generated content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e887e5b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction completed and saved to features.csv\n",
      "\n",
      "Feature Summary:\n",
      "         word_count  sentence_count  avg_word_length  avg_sentence_length  \\\n",
      "count     81.000000       81.000000        81.000000            81.000000   \n",
      "mean   12635.074074        0.839506         5.907426         12635.074074   \n",
      "std    14197.791523        0.369350         2.768334         14197.791523   \n",
      "min        0.000000        0.000000         0.000000             0.000000   \n",
      "25%     1510.000000        1.000000         5.794043          1510.000000   \n",
      "50%     6597.000000        1.000000         6.703314          6597.000000   \n",
      "75%    22209.000000        1.000000         7.600230         22209.000000   \n",
      "max    51342.000000        1.000000        10.318584         51342.000000   \n",
      "\n",
      "       vocab_diversity  keyword_density  stopword_ratio  long_word_ratio  \\\n",
      "count        81.000000        81.000000       81.000000        81.000000   \n",
      "mean          0.054874         0.046522        0.000453         0.417980   \n",
      "std           0.039817         0.039750        0.001381         0.212179   \n",
      "min           0.000000         0.000000        0.000000         0.000000   \n",
      "25%           0.028184         0.031289        0.000000         0.355576   \n",
      "50%           0.047078         0.042445        0.000000         0.499795   \n",
      "75%           0.086187         0.060089        0.000000         0.569057   \n",
      "max           0.179310         0.296587        0.011241         0.702908   \n",
      "\n",
      "       flesch_reading_ease  gunning_fog_index  syllables_per_word  \\\n",
      "count            81.000000          81.000000           81.000000   \n",
      "mean         -12814.156096        5065.435524            1.929021   \n",
      "std           14401.641266        5680.273833            0.914150   \n",
      "min          -52123.788152           0.000000            0.000000   \n",
      "25%          -22516.152546         618.437086            1.847479   \n",
      "50%           -6687.097080        2653.521843            2.198187   \n",
      "75%           -1542.749570        8895.578927            2.525869   \n",
      "max               0.000000       20553.495883            3.400759   \n",
      "\n",
      "       readability_score  readability_proxy  noun_ratio  verb_ratio  \\\n",
      "count               81.0          81.000000   81.000000   81.000000   \n",
      "mean                 0.0          16.049383    0.492667    0.124991   \n",
      "std                  0.0          36.935045    0.223677    0.065108   \n",
      "min                  0.0           0.000000    0.000000    0.000000   \n",
      "25%                  0.0           0.000000    0.533387    0.111150   \n",
      "50%                  0.0           0.000000    0.583280    0.138586   \n",
      "75%                  0.0           0.000000    0.614497    0.160431   \n",
      "max                  0.0         100.000000    0.718398    0.325940   \n",
      "\n",
      "       adj_ratio  adv_ratio  \n",
      "count  81.000000  81.000000  \n",
      "mean    0.171776   0.025528  \n",
      "std     0.081756   0.019277  \n",
      "min     0.000000   0.000000  \n",
      "25%     0.174154   0.008299  \n",
      "50%     0.191054   0.024595  \n",
      "75%     0.212500   0.039544  \n",
      "max     0.371497   0.073525  \n"
     ]
    }
   ],
   "source": [
    "# Load extracted content\n",
    "content_df = pd.read_csv('../data/extracted_content.csv')\n",
    "\n",
    "# Normalize text input to avoid NaN/float issues during tokenization\n",
    "def normalize_text(text):\n",
    "    if pd.isna(text):\n",
    "        return ''\n",
    "    return str(text)\n",
    "\n",
    "def extract_statistical_features(text):\n",
    "    \"\"\"\n",
    "    Extract statistical features from text, including keyword density and additional ratios\n",
    "    \"\"\"\n",
    "    text = normalize_text(text)\n",
    "    words = word_tokenize(text) if text else []\n",
    "    sentences = sent_tokenize(text) if text else []\n",
    "    \n",
    "    # Basic counts\n",
    "    word_count = len(words)\n",
    "    sentence_count = len(sentences)\n",
    "    avg_word_length = np.mean([len(w) for w in words]) if words else 0\n",
    "    avg_sentence_length = word_count / sentence_count if sentence_count > 0 else 0\n",
    "    \n",
    "    # Vocabulary richness\n",
    "    unique_words = len(set(words))\n",
    "    vocab_diversity = unique_words / word_count if word_count > 0 else 0\n",
    "    \n",
    "    # Stopword ratio and keyword density\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stopword_count = sum(1 for w in words if w.lower() in stop_words)\n",
    "    stopword_ratio = (stopword_count / word_count) if word_count > 0 else 0.0\n",
    "    \n",
    "    content_words = [w for w in words if w.isalpha() and w.lower() not in stop_words]\n",
    "    if word_count > 0 and len(content_words) > 0:\n",
    "        values, counts = np.unique([w.lower() for w in content_words], return_counts=True)\n",
    "        max_freq = counts.max() if counts.size > 0 else 0\n",
    "        keyword_density = max_freq / max(1, word_count)\n",
    "    else:\n",
    "        keyword_density = 0.0\n",
    "    \n",
    "    # Long word ratio (>=7 chars)\n",
    "    long_word_ratio = (sum(1 for w in words if len(w) >= 7) / word_count) if word_count > 0 else 0.0\n",
    "    \n",
    "    return {\n",
    "        'word_count': word_count,\n",
    "        'sentence_count': sentence_count,\n",
    "        'avg_word_length': avg_word_length,\n",
    "        'avg_sentence_length': avg_sentence_length,\n",
    "        'vocab_diversity': vocab_diversity,\n",
    "        'keyword_density': float(keyword_density),\n",
    "        'stopword_ratio': float(stopword_ratio),\n",
    "        'long_word_ratio': float(long_word_ratio)\n",
    "    }\n",
    "\n",
    "def extract_readability_features(text):\n",
    "    \"\"\"\n",
    "    Calculate readability metrics and proxies. Adds readability_proxy for more stable scoring.\n",
    "    \"\"\"\n",
    "    text = normalize_text(text)\n",
    "    words = word_tokenize(text) if text else []\n",
    "    sentences = sent_tokenize(text) if text else []\n",
    "    \n",
    "    # Count syllables (approximate)\n",
    "    def count_syllables(word):\n",
    "        return max(1, len(re.findall(r'[aiouy]+e*|e(?!d$|ly).|[td]ed|le$', word.lower())))\n",
    "    \n",
    "    total_syllables = sum(count_syllables(w) for w in words)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    if len(sentences) > 0 and len(words) > 0:\n",
    "        # Flesch Reading Ease\n",
    "        flesch = 206.835 - 1.015 * (len(words) / len(sentences)) - 84.6 * (total_syllables / len(words))\n",
    "        \n",
    "        # Gunning Fog Index\n",
    "        complex_words = sum(1 for word in words if count_syllables(word) >= 3)\n",
    "        fog = 0.4 * ((len(words) / len(sentences)) + 100 * (complex_words / len(words)))\n",
    "    else:\n",
    "        flesch = 0\n",
    "        fog = 0\n",
    "    \n",
    "    readability_score = float(np.clip(flesch, 0, 100))\n",
    "    \n",
    "    # A smoother readability proxy using avg sentence length and syllables/word\n",
    "    words_len = len(words)\n",
    "    syllables_per_word = (total_syllables / words_len) if words_len > 0 else 0.0\n",
    "    avg_sentence_length = (words_len / len(sentences)) if len(sentences) > 0 else 0.0\n",
    "    # Scale to 0-100 (lower ASL and SPW -> higher score)\n",
    "    readability_proxy = 100.0 - np.clip(avg_sentence_length * 1.5 + syllables_per_word * 20.0, 0, 100)\n",
    "    \n",
    "    return {\n",
    "        'flesch_reading_ease': flesch,\n",
    "        'gunning_fog_index': fog,\n",
    "        'syllables_per_word': syllables_per_word,\n",
    "        'readability_score': readability_score,\n",
    "        'readability_proxy': float(readability_proxy)\n",
    "    }\n",
    "\n",
    "def extract_language_features(text):\n",
    "    \"\"\"\n",
    "    Extract language pattern features. Falls back gracefully if POS tagger resources are missing.\n",
    "    \"\"\"\n",
    "    text = normalize_text(text)\n",
    "    words = word_tokenize(text) if text else []\n",
    "    if not words:\n",
    "        return {'noun_ratio': 0.0, 'verb_ratio': 0.0, 'adj_ratio': 0.0, 'adv_ratio': 0.0}\n",
    "    \n",
    "    # Try POS tagging with resilience to missing resources\n",
    "    pos_tags = []\n",
    "    try:\n",
    "        pos_tags = nltk.pos_tag(words, lang='eng')\n",
    "    except LookupError:\n",
    "        try:\n",
    "            nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "            nltk.download('averaged_perceptron_tagger_eng', quiet=True)\n",
    "            pos_tags = nltk.pos_tag(words, lang='eng')\n",
    "        except Exception:\n",
    "            return {'noun_ratio': 0.0, 'verb_ratio': 0.0, 'adj_ratio': 0.0, 'adv_ratio': 0.0}\n",
    "    \n",
    "    total_tags = len(pos_tags)\n",
    "    if total_tags == 0:\n",
    "        return {'noun_ratio': 0.0, 'verb_ratio': 0.0, 'adj_ratio': 0.0, 'adv_ratio': 0.0}\n",
    "    \n",
    "    return {\n",
    "        'noun_ratio': sum(1 for _, tag in pos_tags if tag.startswith('NN')) / total_tags,\n",
    "        'verb_ratio': sum(1 for _, tag in pos_tags if tag.startswith('VB')) / total_tags,\n",
    "        'adj_ratio': sum(1 for _, tag in pos_tags if tag.startswith('JJ')) / total_tags,\n",
    "        'adv_ratio': sum(1 for _, tag in pos_tags if tag.startswith('RB')) / total_tags\n",
    "    }\n",
    "\n",
    "# Extract features for each document\n",
    "features_data = []\n",
    "for idx, row in content_df.iterrows():\n",
    "    text = normalize_text(row['main_content'])\n",
    "    # Combine all features\n",
    "    features = {\n",
    "        'url': row['url'],\n",
    "        **extract_statistical_features(text),\n",
    "        **extract_readability_features(text),\n",
    "        **extract_language_features(text)\n",
    "    }\n",
    "    features_data.append(features)\n",
    "\n",
    "# Save features\n",
    "features_df = pd.DataFrame(features_data)\n",
    "features_df.to_csv('../data/features.csv', index=False)\n",
    "print(\"Feature extraction completed and saved to features.csv\")\n",
    "\n",
    "# Display feature summary\n",
    "print(\"\\nFeature Summary:\")\n",
    "print(features_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57173e5e",
   "metadata": {},
   "source": [
    "# 4. Content Quality Analysis\n",
    "\n",
    "Now we'll implement content quality metrics and train a model to detect low-quality or AI-generated content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f59a8ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quality scores calculated and saved to features.csv\n"
     ]
    }
   ],
   "source": [
    "# Load features\n",
    "features_df = pd.read_csv('../data/features.csv')\n",
    "\n",
    "# Calculate quality score based on features (use smoother readability_proxy)\n",
    "def calculate_quality_score(row):\n",
    "    word_count_score = min(row['word_count'] / 1000, 1.0)  # Normalize to 0-1\n",
    "    keyword_density_score = 1 - min(row['keyword_density'] * 10, 1.0)  # Lower is better\n",
    "    readability_component = max(min(row.get('readability_proxy', 0.0) / 100, 1.0), 0.0)\n",
    "    \n",
    "    # Combine scores with weights\n",
    "    return (word_count_score * 0.25 + \n",
    "            keyword_density_score * 0.35 + \n",
    "            readability_component * 0.40) * 100\n",
    "\n",
    "features_df['seo_score'] = features_df.apply(calculate_quality_score, axis=1)\n",
    "\n",
    "# Save updated features with quality scores\n",
    "features_df.to_csv('../data/features.csv', index=False)\n",
    "print(\"Quality scores calculated and saved to features.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa5f17b",
   "metadata": {},
   "source": [
    "# 5. Duplicate Detection\n",
    "\n",
    "We'll use TF-IDF and cosine similarity to detect duplicate or near-duplicate content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a65a06ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating TF-IDF vectors...\n",
      "\n",
      "Finding optimal number of clusters...\n",
      "Silhouette score for k=2: 0.0493\n",
      "Silhouette score for k=3: 0.0616\n",
      "Silhouette score for k=4: 0.0604\n",
      "Silhouette score for k=5: 0.0600\n",
      "Silhouette score for k=6: 0.0466\n",
      "Silhouette score for k=7: 0.0777\n",
      "Silhouette score for k=8: 0.0898\n",
      "Silhouette score for k=9: 0.0874\n",
      "Silhouette score for k=10: 0.0874\n",
      "\n",
      "Optimal number of clusters: 8\n",
      "\n",
      "Identifying duplicate pairs...\n",
      "\n",
      "Found 4 potential duplicate pairs\n",
      "Results saved to duplicates.csv\n",
      "\n",
      "Cluster Statistics:\n",
      "Average cluster size: 8.50\n",
      "Largest cluster size: 15\n",
      "Number of clusters with potential duplicates: 8\n"
     ]
    }
   ],
   "source": [
    "# Load content for duplicate detection\n",
    "content_df = pd.read_csv('../data/extracted_content.csv')\n",
    "\n",
    "# Clean and normalize text to avoid NaN issues\n",
    "content_df['main_content'] = content_df['main_content'].fillna('').astype(str)\n",
    "non_empty_df = content_df[content_df['main_content'].str.strip() != ''].reset_index(drop=True)\n",
    "\n",
    "if len(non_empty_df) < 2:\n",
    "    print(\"Not enough non-empty documents for duplicate detection. Skipping clustering.\")\n",
    "    pd.DataFrame(columns=['url1', 'url2', 'similarity_score', 'cluster']).to_csv('../data/duplicates.csv', index=False)\n",
    "else:\n",
    "    # Create TF-IDF vectors\n",
    "    print(\"Generating TF-IDF vectors...\")\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_features=5000,\n",
    "        stop_words='english',\n",
    "        ngram_range=(1, 2),  # Include bigrams\n",
    "        min_df=2,  # Ignore terms that appear in less than 2 documents\n",
    "        max_df=0.95  # Ignore terms that appear in more than 95% of documents\n",
    "    )\n",
    "    tfidf_matrix = vectorizer.fit_transform(non_empty_df['main_content'])\n",
    "\n",
    "    # Determine optimal number of clusters\n",
    "    print(\"\\nFinding optimal number of clusters...\")\n",
    "    silhouette_scores = []\n",
    "    max_k = min(11, len(non_empty_df))\n",
    "    K = range(2, max_k)  # Test K from 2 up to max_k-1\n",
    "    for k in K:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "        cluster_labels = kmeans.fit_predict(tfidf_matrix)\n",
    "        silhouette_avg = silhouette_score(tfidf_matrix, cluster_labels)\n",
    "        silhouette_scores.append(silhouette_avg)\n",
    "        print(f\"Silhouette score for k={k}: {silhouette_avg:.4f}\")\n",
    "\n",
    "    # Select optimal K\n",
    "    optimal_k = K[np.argmax(silhouette_scores)] if len(silhouette_scores) > 0 else 2\n",
    "    print(f\"\\nOptimal number of clusters: {optimal_k}\")\n",
    "\n",
    "    # Perform clustering with optimal K\n",
    "    kmeans = KMeans(n_clusters=optimal_k, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(tfidf_matrix)\n",
    "\n",
    "    # Find potential duplicates within clusters\n",
    "    print(\"\\nIdentifying duplicate pairs...\")\n",
    "    duplicate_pairs = []\n",
    "    similarity_threshold = 0.8\n",
    "\n",
    "    for cluster in range(optimal_k):\n",
    "        # Get documents in the current cluster\n",
    "        cluster_docs = np.where(cluster_labels == cluster)[0]\n",
    "        \n",
    "        if len(cluster_docs) > 1:\n",
    "            # Calculate similarities between all documents in the cluster\n",
    "            cluster_vectors = tfidf_matrix[cluster_docs]\n",
    "            similarities = cosine_similarity(cluster_vectors)\n",
    "            \n",
    "            # Find similar pairs\n",
    "            for i in range(len(cluster_docs)):\n",
    "                for j in range(i + 1, len(cluster_docs)):\n",
    "                    if similarities[i][j] > similarity_threshold:\n",
    "                        doc1_idx = cluster_docs[i]\n",
    "                        doc2_idx = cluster_docs[j]\n",
    "                        duplicate_pairs.append({\n",
    "                            'url1': non_empty_df.iloc[doc1_idx]['url'],\n",
    "                            'url2': non_empty_df.iloc[doc2_idx]['url'],\n",
    "                            'similarity_score': similarities[i][j],\n",
    "                            'cluster': cluster\n",
    "                        })\n",
    "\n",
    "    # Save duplicate pairs\n",
    "    duplicates_df = pd.DataFrame(duplicate_pairs)\n",
    "    duplicates_df.to_csv('../data/duplicates.csv', index=False)\n",
    "    print(f\"\\nFound {len(duplicate_pairs)} potential duplicate pairs\")\n",
    "    print(\"Results saved to duplicates.csv\")\n",
    "\n",
    "    # Print cluster statistics\n",
    "    print(\"\\nCluster Statistics:\")\n",
    "    cluster_sizes = pd.Series(cluster_labels).value_counts()\n",
    "    print(f\"Average cluster size: {cluster_sizes.mean():.2f}\")\n",
    "    print(f\"Largest cluster size: {cluster_sizes.max()}\")\n",
    "    print(f\"Number of clusters with potential duplicates: {len(cluster_sizes[cluster_sizes > 1])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1aa3a41",
   "metadata": {},
   "source": [
    "# 6. Model Training and Evaluation\n",
    "\n",
    "Train a Random Forest model to classify content quality based on our features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "928357a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label balance (low/medium/high):\n",
      "seo_score\n",
      "low       27\n",
      "medium    27\n",
      "high      27\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Classification Performance (low/medium/high):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         low      1.000     0.833     0.909         6\n",
      "      medium      0.857     1.000     0.923         6\n",
      "        high      1.000     1.000     1.000         5\n",
      "\n",
      "    accuracy                          0.941        17\n",
      "   macro avg      0.952     0.944     0.944        17\n",
      "weighted avg      0.950     0.941     0.941        17\n",
      "\n",
      "\n",
      "Feature Importance (permutation):\n",
      "               feature  importance\n",
      "1      keyword_density    0.552941\n",
      "5       stopword_ratio    0.029412\n",
      "4      long_word_ratio    0.017647\n",
      "0           word_count    0.000000\n",
      "2    readability_proxy    0.000000\n",
      "3  avg_sentence_length    0.000000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Prepare features for modeling (expanded feature set)\n",
    "feature_cols = ['word_count', 'keyword_density', 'readability_proxy', 'avg_sentence_length', 'long_word_ratio', 'stopword_ratio']\n",
    "# Backward compatibility if columns missing\n",
    "feature_cols = [c for c in feature_cols if c in features_df.columns]\n",
    "X = features_df[feature_cols]\n",
    "\n",
    "# Create 3-class labels from seo_score using tertiles (low/medium/high) with robust fallback\n",
    "labels = None\n",
    "try:\n",
    "    labels = pd.qcut(features_df['seo_score'], q=3, labels=['low', 'medium', 'high'])\n",
    "except ValueError:\n",
    "    # Fallback if too many ties; use unique quantiles\n",
    "    qs = features_df['seo_score'].quantile([0.33, 0.66]).values\n",
    "    qs = np.unique(qs)\n",
    "    if len(qs) == 1:\n",
    "        # Degenerate: fall back to median-based binary then map to low/high\n",
    "        threshold = features_df['seo_score'].median()\n",
    "        labels = np.where(features_df['seo_score'] > threshold, 'high', 'low')\n",
    "    else:\n",
    "        bins = [-np.inf, qs[0], qs[-1], np.inf]\n",
    "        labels = pd.cut(features_df['seo_score'], bins=bins, labels=['low', 'medium', 'high'], include_lowest=True)\n",
    "\n",
    "y = pd.Series(labels)\n",
    "\n",
    "# Show class balance\n",
    "print(\"Label balance (low/medium/high):\")\n",
    "print(y.value_counts())\n",
    "\n",
    "# If single-class labels, fall back to regression on seo_score\n",
    "if y.nunique() < 2:\n",
    "    print(\"\\nSingle-class labels detected. Falling back to regression to predict seo_score.\")\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.metrics import r2_score, mean_absolute_error\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, features_df['seo_score'], test_size=0.2, random_state=42)\n",
    "    reg = RandomForestRegressor(n_estimators=300, random_state=42, max_depth=None, min_samples_split=2, min_samples_leaf=1)\n",
    "    reg.fit(X_train, y_train)\n",
    "    y_pred = reg.predict(X_test)\n",
    "\n",
    "    print(\"\\nRegression Performance:\")\n",
    "    print(f\"R^2: {r2_score(y_test, y_pred):.3f}\")\n",
    "    print(f\"MAE: {mean_absolute_error(y_test, y_pred):.3f}\")\n",
    "\n",
    "    # Export a consistent model variable for saving\n",
    "    model = reg\n",
    "else:\n",
    "    # Stratified split for multiclass classification\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    # Train classifier with slightly stronger settings\n",
    "    base_rf = RandomForestClassifier(\n",
    "        n_estimators=400,\n",
    "        max_depth=None,\n",
    "        min_samples_split=2,\n",
    "        min_samples_leaf=1,\n",
    "        max_features='sqrt',\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    # Calibrate probabilities with stratified CV\n",
    "    skf = StratifiedKFold(n_splits=min(5, y.value_counts().min()), shuffle=True, random_state=42)\n",
    "    clf = CalibratedClassifierCV(estimator=base_rf, cv=skf, method='sigmoid')\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(\"\\nClassification Performance (low/medium/high):\")\n",
    "    print(classification_report(y_test, y_pred, digits=3, labels=['low','medium','high']))\n",
    "\n",
    "    # Confusion matrix (best-effort; safe if display backend not available)\n",
    "    try:\n",
    "        ConfusionMatrixDisplay.from_predictions(y_test, y_pred, display_labels=['low','medium','high'])\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Feature importance via permutation (model-agnostic)\n",
    "    try:\n",
    "        perm = permutation_importance(clf, X_test, y_test, n_repeats=10, random_state=42, n_jobs=-1)\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'feature': feature_cols,\n",
    "            'importance': perm.importances_mean\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        print(\"\\nFeature Importance (permutation):\")\n",
    "        print(feature_importance)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Export a consistent model variable for saving\n",
    "    model = clf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49497dc4",
   "metadata": {},
   "source": [
    "# 7. Save Results and Model\n",
    "\n",
    "Finally, we'll save our trained model for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8ca74090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ../models/quality_model.pkl\n",
      "\n",
      "Project Summary:\n",
      "Total URLs processed: 81\n",
      "Features extracted: 81\n",
      "Duplicate pairs found: 4\n",
      "Average content quality score: 46.92\n",
      "Median Word Count: 6597\n"
     ]
    }
   ],
   "source": [
    "# Save the trained model\n",
    "model_path = '../models/quality_model.pkl'\n",
    "with open(model_path, 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "print(f\"Model saved to {model_path}\")\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\nProject Summary:\")\n",
    "print(f\"Total URLs processed: {len(content_df)}\")\n",
    "print(f\"Features extracted: {len(features_df)}\")\n",
    "print(f\"Duplicate pairs found: {len(pd.read_csv('../data/duplicates.csv'))}\")\n",
    "print(f\"Average content quality score: {features_df['seo_score'].mean():.2f}\")\n",
    "print(f\"Median Word Count: {features_df['word_count'].median():.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48149573",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
