# ğŸ“˜ SEO Content Quality & Duplicate Detector

## ğŸ” Project Overview

**SEO Content Quality & Duplicate Detector** is a machine learningâ€“powered web content analysis system that evaluates webpage quality, readability, structure, and SEO effectiveness. It also detects near-duplicate or thin content across multiple pages â€” helping users and organizations maintain high-quality, original, and search-friendly content.

This project was developed as part of the **Data Science Assignment** for SEO Quality Assessment & Duplicate Detection.

---

## ğŸ¯ Objectives

The main goal of this project is to:

- **Analyze** webpage content (HTML or URL-based) for SEO and readability quality.
- **Detect** near-duplicate or low-value ("thin") content.
- **Build** a machine learning pipeline that classifies pages into **High / Medium / Low** quality levels.
- **Provide** a real-time analysis dashboard using Streamlit.

---

## ğŸ’¡ Why It's Helpful

This project helps:

- **SEO teams** identify and improve underperforming or repetitive content.
- **Content writers** measure the clarity and structure of their writing.
- **Developers / Analysts** automate web content evaluation using NLP and ML techniques.
- **Organizations** ensure that all published material meets a consistent quality standard before going live.

By automating SEO and readability checks, users can make data-driven decisions that improve rankings and user engagement.

---

## âš™ï¸ Tech Stack

| Component | Technology |
|-----------|------------|
| Language | Python 3.9+ |
| Web App | Streamlit |
| ML & NLP | scikit-learn, TF-IDF, cosine similarity |
| Text Parsing | BeautifulSoup |
| Readability | textstat (Flesch Reading Ease) |
| Data | Pre-scraped HTML dataset (60â€“70 URLs) |
| Visualization | Streamlit metrics, JSON export |

---

## ğŸ§± Project Structure

```
seo-content-detector/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ data.csv                  # Original dataset (URLs + HTML)
â”‚   â”œâ”€â”€ extracted_content.csv     # Parsed clean content
â”‚   â”œâ”€â”€ features.csv              # Feature-engineered data
â”‚   â””â”€â”€ duplicates.csv            # Duplicate pairs detected
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ seo_pipeline.ipynb        # Main analysis notebook (core pipeline)
â”‚
â”œâ”€â”€ streamlit_app/
â”‚   â”œâ”€â”€ app.py                    # Streamlit web app
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ parser.py             # HTML parsing logic
â”‚   â”‚   â”œâ”€â”€ features.py           # NLP feature extraction
â”‚   â”‚   â””â”€â”€ scorer.py             # Model scoring + labeling
â”‚   â””â”€â”€ models/
â”‚       â””â”€â”€ quality_model.pkl     # Trained content quality classifier
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ§© Features Implemented

### âœ… 1. HTML Parsing & Extraction
- Parses `<title>`, `<p>`, `<article>`, and `<main>` sections.
- Cleans markup and counts words.

### âœ… 2. Feature Engineering
- Word count, sentence count, Flesch Reading Ease score.
- TF-IDF keyword extraction (Top 5 keywords).

### âœ… 3. Duplicate Detection
- Cosine similarity between TF-IDF vectors.
- Flags pages with similarity > 0.8 as near-duplicates.

### âœ… 4. Quality Scoring Model
- Combines rule-based labeling and ML classifier.
- Classifies content as **Low / Medium / High** quality.

### âœ… 5. Real-Time Streamlit App
- Input a live URL â†’ fetch â†’ analyze â†’ display SEO insights.
- Compares against dataset â†’ lists top similar high-quality pages.

**Sections:**
- ğŸ§¾ Readability Breakdown
- ğŸ” Keyword Analysis
- ğŸ“‰ Structure Metrics
- ğŸ—£ Tone & Voice
- ğŸ“ˆ Recommendations & Improvements

---

## ğŸ§  How It Works

1. **Input** â€“ The user provides a webpage URL or dataset.
2. **Processing** â€“ HTML content is parsed to extract text and compute NLP metrics.
3. **Feature Extraction** â€“ TF-IDF, readability, and text statistics are generated.
4. **Model Scoring** â€“ The trained ML model predicts content quality.
5. **Output** â€“
   - Overall quality (High / Medium / Low)
   - Detailed readability, keyword, and tone metrics
   - Recommended improvements
   - Similar high-quality content suggestions

---

## ğŸ§‘â€ğŸ’» How to Run Locally

### Step 1 â€” Clone the repo

```bash
git clone https://github.com/yourusername/seo-content-detector.git
cd seo-content-detector
```

### Step 2 â€” Install dependencies

```bash
pip install -r requirements.txt
```

### Step 3 â€” Run the Streamlit app

```bash
cd streamlit_app
streamlit run app.py
```

### Step 4 â€” Analyze a webpage

1. Enter a URL (e.g., `https://example.com/blog`)
2. View readability, SEO metrics, tone, and quality label.
3. Download a JSON report of the analysis.

---

## ğŸ“Š Example Output

### JSON Result

```json
{
  "url": "https://example.com/article",
  "word_count": 1450,
  "flesch_reading_ease": 65.2,
  "quality_label": "High",
  "is_thin": false,
  "similar_to": [
    {"url": "https://example.com/related-article", "similarity": 0.82}
  ]
}
```

---

## ğŸ§¾ Key Insights

- Content with **>1500 words** and readability between **50â€“70** tends to be rated "High."
- **Keyword diversity** and balanced tone strongly correlate with better SEO scores.
- The system identifies redundant or duplicated pages to maintain unique, valuable content.

---

## ğŸš€ Results & Impact

- **High-quality detection accuracy:** ~78% (vs baseline 64%)
- **Duplicate detection threshold:** 0.8 cosine similarity
- **Thin content rate:** ~10% across dataset

The model provides actionable insights for improving content performance â€” helping teams focus on clarity, depth, and originality.

---

## ğŸ§© Future Enhancements

- Integrate **BERT embeddings** for deeper semantic similarity.
- Add **sentiment and topic modeling** for tone refinement.
- Visualize **similarity heatmaps** and readability distribution.
- **API endpoints** for automated batch analysis.

---

## âœ¨ Credits

**Developed by Anshu (Aptico EdTech)**  
As part of the **Data Science Assignment** â€“ SEO Content Quality & Duplicate Detector.
